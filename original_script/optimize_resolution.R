#!/usr/bin/env Rscript

###############################################################################
## Authors: Reuben Thomas, Natalie Elphick and Ayushi Agrawal
##
## Script Goal: Optimize resolution parameter; check silhouette score
## distribution and decide the optimal resolution
##
## Usage example:
## Rscript optimize_res.R \
##  --input 'input_seurat_object.RDS' \  # Seurat object with multiple samples
##  --output '/output_directory' \       # Location for output files
##  --mode "scRNA" \                     # Is the input scRNA or CyTOF?
##  --output_prefix "outputs_scRNA_" \   # Prefix for output files
##  --ndim 20 \                          # Number of PCs to use
##  --metadata "sample"                  # Name of sample identifier
##  --within_batch_var "batch"           # Name of batch variable
##
## Run "Rscript optimize_res.R --help" for more information
###############################################################################


# Get input arguments -----------------------------------------------------
library(optparse)
option_list <- list(
  make_option(c("-i", "--input"),
    action = "store", default = NA, type = "character",
    help = "Input Seurat object in RDS format (required)"
  ),
  make_option(c("-o", "--output"),
    action = "store", default = NA, type = "character",
    help = "Output directory, will create if it doesn't exist (required)"
  ),
  make_option(c("-c", "--cores"),
    action = "store", default = 2, type = "numeric",
    help = "Number of cores to use, [default %default]"
  ),
  make_option(c("--mode"),
    action = "store", default = NA, type = "character",
    help = "Input data type vaid options are 'scRNA' or 'CyTOF' (required)"
  ),
  make_option(c("--output_prefix"),
    action = "store",
    default = "optimize_resolution",
    type = "character",
    help = "Prefix for output files, [default %default]"
  ),
  make_option(c("-n", "--ndim"),
    action = "store", default = NA, type = "numeric",
    help = "Number of principal components to use (required)"
  ),
  make_option(c("-m", "--metadata"),
    action = "store", default = NA, type = "character",
    help = "The metadata value for sample in the Seurat object (required)"
  ),
  make_option(c("--within_batch_var"),
    action = "store", default = NA, type = "character",
    help = "The metadata value for a batching variable that if provided, 
    will be used to train the model on samples within the same batch. Only works if samples are not spread
    across batches (optional)"
  )
)

opt <- parse_args(OptionParser(option_list = option_list))

# Check if required args are provided
if (is.na(opt$metadata) | is.na(opt$input) | is.na(opt$output) | is.na(opt$ndim) | is.na(opt$mode)) {
  stop("Missing one or more required arguments")
}

# Check if mode argument is a valid option
if (opt$mode != "scRNA" & opt$mode != "CyTOF") {
  stop("Invalid option for --mode, must be scRNA or CyTOF")
}


# Load required packages --------------------------------------------------
library(tidyverse)
library(Seurat)
library(ranger)
library(cluster)
library(foreach)
library(doParallel)
library(future)


# Prevent the Rplots.pdf from being autogenerated
pdf(NULL)

# Leave one core free
nCores <- opt$cores - 1

# Register the cluster for foreach
registerDoParallel(cores = nCores)
getDoParWorkers()
getDoParName()
set.seed(1234)




# create the results folders
if (!(dir.exists(opt$output))) {
  dir.create(opt$output)
}



# Initialize variables & read in data -------------------------------------

# Read in the Seurat object
input_rds <- readRDS(opt$input)

# Set number of PCs to use
ndim <- opt$ndim

# Set range of resolutions to test
res_range <- c(0.02, 0.04, 0.06, 0.08, 0.1, 0.2, 0.4, 0.6, 0.8, 1, 1.2)

# Get sample information
sample_obj_id <- opt$metadata
n_samples <- length(unique(input_rds@meta.data[[sample_obj_id]]))
sample_names <- as.vector((unique(input_rds@meta.data[[sample_obj_id]])))




# Main function -----------------------------------------------------------

# CyTOF main function
run_rf_cytof <- function(sample, res) {
  this_sample <- sample_names[sample]
  print(paste0("Working on: ", this_sample, "  Resolution: ", res))

  # Extract the data and reformat for random forest
  df <- GetAssayData(input_rds, slot = "counts", assay = "RNA") %>%
    as.data.frame() %>%
    t() %>%
    as_tibble(rownames = "CellID") %>%
    mutate(SampleID = input_rds@meta.data[[sample_obj_id]]) %>%
    select(CellID, SampleID, everything())

  # Fix illegal column names
  names(df) <- gsub("-", "_", names(df), fixed = TRUE)

  # Subset the Seurat object to cluster the training samples only
  train_cells <- colnames(input_rds)[which(input_rds[[]][opt$metadata] != this_sample)]
  train_seurat <- subset(input_rds, cells = train_cells)

  # Subset the training and testing samples
  train_df <- df %>% filter(SampleID != this_sample)
  test_df <- df %>% filter(SampleID == this_sample)

  # Cluster the training data
  train_seurat <- ScaleData(train_seurat, features = NULL)
  train_seurat <- FindVariableFeatures(train_seurat,
    selection.method = "vst", nfeatures = ndim
  )
  train_seurat <- RunPCA(train_seurat, npcs = ndim, approx = FALSE)
  train_seurat <- FindNeighbors(object = train_seurat, dims = 1:ndim)

  train_seurat <- FindClusters(object = train_seurat, resolution = res)

  # Label clusters in the training data
  train_df <- train_df %>%
    mutate(clusters = train_seurat@meta.data$seurat_clusters) %>%
    as.data.frame()

  # Train the model
  rf <- ranger(as.factor(clusters) ~ .,
    data = train_df[, 3:ncol(train_df)],
    num.trees = 1000,
    write.forest = TRUE,
    num.threads = 1
  )

  # Predict on the hold out sample
  predicted <- predict(rf, test_df[, 3:ncol(test_df)])
  predicted <- predictions(predicted)
  predicted_clusters_table <- table(predicted)

  trained_clusters_table <- table(train_seurat@meta.data$seurat_clusters)

  # Get silhouette scores
  sil <- silhouette(
    as.numeric(as.character(predicted)),
    dist(test_df[, 3:ncol(test_df)])
  )
  # Set values to NA if there is only one cluster
  if (class(sil) == "logical") {
    sil_mean <- NA
    sil_group_mean <- NA
  } else {
    sil_summary <- summary(sil)
    sil_mean <- sil_summary$avg.width
    sil_group_mean <- mean(sil_summary$clus.avg.widths)
  }

  tibble(
    resolution = res,
    test_sample = this_sample,
    avg_width = sil_mean,
    cluster_avg_widths = sil_group_mean,
    n_predicted_clusters = length(unique(as.character(predicted))),
    min_predicted_cell_per_cluster = min(predicted_clusters_table),
    max_predicted_cell_per_cluster = max(predicted_clusters_table),
    n_trained_clusters = length(trained_clusters_table),
    min_trained_cell_per_cluster = min(trained_clusters_table),
    max_trained_cell_per_cluster = max(trained_clusters_table)
  )
}

# scRNA main function
run_rf_scrna <- function(res, train_seurat, test_seurat) {
  print(paste0("Working on resolution: ", res))
  
  #data frame to record silhouette score of training data
  #based on the actual cluster assignments of each of the training samples
  this_sil_actual_train_df <- data.frame(resolution = numeric(),
                                         sample_test = character(),
                                         sample_train = character(),
                                         avg_width = numeric(),
                                         cluster_avg_widths = numeric(),
                                         n_predicted_clusters = numeric(),
                                         min_predicted_cell_per_cluster = numeric(),
                                         max_predicted_cell_per_cluster = numeric(),
                                         n_trained_clusters = numeric(),
                                         min_trained_cell_per_cluster = numeric(),
                                         max_trained_cell_per_cluster = numeric())
  
  #data frame to record silhouette score of training data 
  #based on the predicted cell membership of the training samples
  this_sil_predict_train_df <- data.frame(resolution = numeric(),
                                          sample_test = character(),
                                          sample_train = character(),
                                          avg_width = numeric(),
                                          cluster_avg_widths = numeric(),
                                          n_predicted_clusters = numeric(),
                                          min_predicted_cell_per_cluster = numeric(),
                                          max_predicted_cell_per_cluster = numeric(),
                                          n_trained_clusters = numeric(),
                                          min_trained_cell_per_cluster = numeric(),
                                          max_trained_cell_per_cluster = numeric())
  
  # Cluster training samples
  train_seurat <- FindClusters(
    object = train_seurat,
    resolution = res,
    verbose = FALSE
  )
  
  # Features present in both the training variable features and test sample
  common_features <- intersect(
    rownames(test_seurat@assays[["SCT"]]@scale.data),
    VariableFeatures(train_seurat)
  )
  # Extract the loadings for common features from the training data
  loadings_common_features <- Loadings(train_seurat[["pca"]]) %>%
    as_tibble(rownames = "Features") %>%
    filter(Features %in% common_features) %>%
    as.matrix()
  
  rownames(loadings_common_features) <- loadings_common_features[, 1]
  loadings_common_features <- loadings_common_features[, -1]
  class(loadings_common_features) <- "numeric"
  
  pca_train_data <- as.matrix(train_seurat[["SCT"]]@scale.data)[common_features, ] %>%
    t() %*% loadings_common_features
  
  
  # Project the cells in the test data onto the PCs
  # identified using the training data
  pca_test_data <- as.matrix(test_seurat[["SCT"]]@scale.data)[common_features, ] %>%
    t() %*% loadings_common_features
  # Free up some memory
  rm(test_seurat)
  
  # Number of dimensions to train with
  train_df <- pca_train_data[, 1:ndim]
  test_df <- pca_test_data[, 1:ndim]
  
  # Label clusters in the training data
  train_df <- train_df %>%
    as.data.frame() %>%
    mutate(clusters = train_seurat@meta.data$seurat_clusters)
  # Get the cluster info for the training data
  trained_clusters_table <- table(train_seurat@meta.data$seurat_clusters)
  
  # Get silhouette scores per sample for train data
  # based on the actual cluster assignments of each of the training samples 
  sample_names_train_data <- as.vector((unique(train_seurat@meta.data[[sample_obj_id]])))
  for(smp_train in 1:length(sample_names_train_data)){
    smp_train_cells <- colnames(train_seurat)[which(train_seurat[[]][sample_obj_id] == sample_names_train_data[smp_train])]
    smp_train_df <- train_df[smp_train_cells,]
    actual_smp_train <- smp_train_df$clusters
    actual_clusters_table_smp_train <- table(actual_smp_train)
    
    sil <- silhouette(
      as.numeric(as.character(actual_smp_train)),
      dist(smp_train_df[,1:ndim])
    )
    # Set values to NA if there is only one cluster
    if (class(sil) == "logical") {
      sil_mean <- NA
      sil_group_mean <- NA
    } else {
      sil_summary <- summary(sil)
      sil_mean <- sil_summary$avg.width
      sil_group_mean <- mean(sil_summary$clus.avg.widths)
    }
    
    this_sil_actual_train_df[nrow(this_sil_actual_train_df)+1,] <- c(res, this_sample, sample_names_train_data[smp_train],
                                                                     sil_mean, sil_group_mean, length(unique(as.character(actual_smp_train))),
                                                                     min(actual_clusters_table_smp_train), max(actual_clusters_table_smp_train),
                                                                     length(trained_clusters_table),min(trained_clusters_table),max(trained_clusters_table))
  }
  
  # Train the model
  rf <- ranger(as.factor(clusters) ~ .,
               data = train_df,
               num.trees = 1000,
               write.forest = TRUE,
               num.threads = 1
  )
  
  # Predict on each of the training samples and get silhouette score of training data 
  #based on the predicted cell membership of the training samples
  for(smp_predict in 1:length(sample_names_train_data)){
    #create a subset data for one sample from the training data
    smp_test_cells <- colnames(train_seurat)[which(train_seurat[[]][sample_obj_id] == sample_names_train_data[smp_predict])]
    smp_test_df <- train_df[smp_test_cells,]
    actual_smp_test <- smp_test_df$clusters
    actual_clusters_table_smp_test <- table(actual_smp_test)
    
    # Predict on one of the training samples
    predicted <- predict(rf, smp_test_df[,1:ndim])
    predicted <- predictions(predicted)
    predicted_clusters_table <- table(predicted)
    
    # Get silhouette scores for test data
    sil <- silhouette(
      as.numeric(as.character(predicted)),
      dist(smp_test_df[,1:ndim])
    )
    # Set values to NA if there is only one cluster
    if (class(sil) == "logical") {
      sil_mean <- NA
      sil_group_mean <- NA
    } else {
      sil_summary <- summary(sil)
      sil_mean <- sil_summary$avg.width
      sil_group_mean <- mean(sil_summary$clus.avg.widths)
    }
    
    this_sil_predict_train_df[nrow(this_sil_predict_train_df)+1,] <- c(res, this_sample, sample_names_train_data[smp_predict],
                                                                       sil_mean, sil_group_mean, length(unique(as.character(predicted))),
                                                                       min(predicted_clusters_table), max(predicted_clusters_table),
                                                                       length(trained_clusters_table),min(trained_clusters_table),max(trained_clusters_table))
  }
  
  rm(train_seurat)
  
  # Predict on the hold out sample
  predicted <- predict(rf, test_df)
  predicted <- predictions(predicted)
  predicted_clusters_table <- table(predicted)
  rm(rf)
  
  # Get silhouette scores for test data
  sil <- silhouette(
    as.numeric(as.character(predicted)),
    dist(test_df)
  )
  # Set values to NA if there is only one cluster
  if (class(sil) == "logical") {
    sil_mean <- NA
    sil_group_mean <- NA
  } else {
    sil_summary <- summary(sil)
    sil_mean <- sil_summary$avg.width
    sil_group_mean <- mean(sil_summary$clus.avg.widths)
  }
  
  
  this_res <- tibble(
    resolution = res,
    test_sample = this_sample,
    avg_width = sil_mean,
    cluster_avg_widths = sil_group_mean,
    n_predicted_clusters = length(unique(as.character(predicted))),
    min_predicted_cell_per_cluster = min(predicted_clusters_table),
    max_predicted_cell_per_cluster = max(predicted_clusters_table),
    n_trained_clusters = length(trained_clusters_table),
    min_trained_cell_per_cluster = min(trained_clusters_table),
    max_trained_cell_per_cluster = max(trained_clusters_table)
  )
  this_all_res <- list(this_res, this_sil_actual_train_df, this_sil_predict_train_df)
  names(this_all_res) <- c("res","sil_actual_train_df","sil_predict_train_df")
  return(this_all_res)
}


if (opt$mode == "scRNA") {
  #data frame to record silhouette score of training data
  #based on the actual cluster assignments of each of the training samples
  sil_actual_train_df <- NULL
  
  #data frame to record silhouette score of training data 
  #based on the predicted cell membership of the training samples
  sil_predict_train_df <- NULL
  
  #data frame to record silhouette score of test data 
  #based on the predicted cell membership of the training samples
  res_tbl <- NULL
  
  for (sam in seq(1:n_samples)) {
    this_sample <- sample_names[sam]
    print(paste0("Working on: ", this_sample))
    
    # If within_batch_var is provided, then use only traning samples from the same batch
    if (!is.na(opt$within_batch_var)) {
      # Get the batch of this_sample
      this_batch <- input_rds@meta.data %>%
      filter(get(sample_obj_id) == this_sample) %>%
      pull(get(opt$within_batch_var)) %>%
      unique()
      
      # Get the type of the within_batch_var
      this_batch_type <- class(input_rds@meta.data[[opt$within_batch_var]])

      if (length(this_batch) > 1) {
        stop("More than one batch found for this sample")
      }
      train_cells <- input_rds@meta.data %>%
      filter(get(sample_obj_id) != this_sample & get(opt$within_batch_var) == this_batch) %>%
      rownames()
      train_seurat <- subset(input_rds, cells = train_cells)
      print(paste0("Using only samples from batch: ", unique(train_seurat@meta.data[[opt$within_batch_var]])))
      # Subset the test sample
      test_cells <- colnames(input_rds)[which(input_rds[[]][sample_obj_id] == this_sample)]
      test_seurat <- subset(input_rds, cells = test_cells)
    } else {

      # Subset the Seurat object to cluster the training samples only
      train_cells <- colnames(input_rds)[which(input_rds[[]][sample_obj_id] != this_sample)]
      train_seurat <- subset(input_rds, cells = train_cells)
      # Subset the test sample
      test_cells <- colnames(input_rds)[which(input_rds[[]][sample_obj_id] == this_sample)]
      test_seurat <- subset(input_rds, cells = test_cells)
    }
   
    # Prep the Seurat objects
    train_seurat <- SCTransform(train_seurat,
                                vst.flavor = "v2",
                                verbose = FALSE
    )
    test_seurat <- SCTransform(test_seurat,
                               vst.flavor = "v2",
                               verbose = FALSE,
                               variable.features.n = length(rownames(test_seurat)),
                               return.only.var.genes = FALSE,
                               min_cells = 1
    )
    
    print(paste0(
      "Found ",
      length(intersect(
        rownames(test_seurat@assays[["SCT"]]@scale.data),
        VariableFeatures(train_seurat)
      )),
      " shared genes between testing and training data"
    ))
    
    train_seurat <- RunPCA(train_seurat,
                           npcs = ndim,
                           verbose = FALSE,
                           assay = "SCT"
    )
    train_seurat <- FindNeighbors(object = train_seurat, dims = 1:ndim, verbose = FALSE)
    
    this_all_res <- foreach(res = res_range, .combine = "rbind") %dopar% {
      run_rf_scrna(
        res = res,
        train_seurat = train_seurat,
        test_seurat = test_seurat
      )
    }
    
    res_tbl <- rbind(res_tbl, do.call(rbind, c(this_all_res[,"res"], make.row.names = F)))
    sil_actual_train_df <- rbind(sil_actual_train_df, do.call(rbind, c(this_all_res[,"sil_actual_train_df"], make.row.names = F)))
    sil_actual_train_df[,4:ncol(sil_actual_train_df)] <- lapply(sil_actual_train_df[,4:ncol(sil_actual_train_df)],as.numeric)
    sil_predict_train_df <- rbind(sil_predict_train_df, do.call(rbind, c(this_all_res[,"sil_predict_train_df"], make.row.names = F)))
    sil_predict_train_df[,4:ncol(sil_predict_train_df)] <- lapply(sil_predict_train_df[,4:ncol(sil_predict_train_df)],as.numeric)
  }
} else {
  res_tbl <- foreach(
    sample = seq(1:n_samples),
    .combine = "rbind"
  ) %:%
    foreach(res = res_range, .combine = "rbind") %dopar% {
      run_rf_cytof(sample = sample, res = res)
    }
}



# Summarize results
results_summary <- res_tbl %>%
  group_by(resolution) %>%
  summarize(
    mean_score = mean(avg_width, na.rm = TRUE),
    variance_score = var(avg_width, na.rm = TRUE),
    standard_error_score = sd(avg_width, na.rm = TRUE) / sqrt(length(avg_width)),
    cluster_mean_score = mean(cluster_avg_widths, na.rm = TRUE),
    cluster_variance_score = var(cluster_avg_widths, na.rm = TRUE),
    cluster_standard_error_score = sd(cluster_avg_widths,na.rm = TRUE) / sqrt(length(avg_width))
  )

sil_actual_train_df <- sil_actual_train_df %>%
  group_by(resolution, sample_train) %>%
  summarize(
    mean_avg_width = mean(avg_width, na.rm = TRUE),
    mean_cluster_avg_widths = mean(cluster_avg_widths, na.rm = TRUE)
  )
results_summary_actual_train_data <- sil_actual_train_df %>%
  group_by(resolution) %>% 
  summarize(
    mean_score = mean(mean_avg_width, na.rm = TRUE),
    variance_score = var(mean_avg_width, na.rm = TRUE),
    standard_error_score = sd(mean_avg_width, na.rm = TRUE) / sqrt(length(mean_avg_width)),
    cluster_mean_score = mean(mean_cluster_avg_widths, na.rm = TRUE),
    cluster_variance_score = var(mean_cluster_avg_widths, na.rm = TRUE),
    cluster_standard_error_score = sd(mean_cluster_avg_widths,na.rm = TRUE) / sqrt(length(mean_avg_width))
  )

sil_predict_train_df <- sil_predict_train_df %>%
  group_by(resolution, sample_train) %>%
  summarize(
    mean_avg_width = mean(avg_width, na.rm = TRUE),
    mean_cluster_avg_widths = mean(cluster_avg_widths, na.rm = TRUE)
  ) 
results_summary_predicted_train_data <- sil_predict_train_df %>%
  group_by(resolution) %>% 
  summarize(
    mean_score = mean(mean_avg_width, na.rm = TRUE),
    variance_score = var(mean_avg_width, na.rm = TRUE),
    standard_error_score = sd(mean_avg_width, na.rm = TRUE) / sqrt(length(mean_avg_width)),
    cluster_mean_score = mean(mean_cluster_avg_widths, na.rm = TRUE),
    cluster_variance_score = var(mean_cluster_avg_widths, na.rm = TRUE),
    cluster_standard_error_score = sd(mean_cluster_avg_widths,na.rm = TRUE) / sqrt(length(mean_avg_width))
  )

# Generate outputs --------------------------------------------------------

write_csv(
  x = res_tbl,
  file = paste0(
    opt$output,
    "/",
    opt$output_prefix,
    "_results_full.csv"
  )
)

write_csv(
  x = results_summary,
  file = paste0(
    opt$output,
    "/",
    opt$output_prefix,
    "_results_summary.csv"
  )
)

write_csv(
  x = sil_actual_train_df,
  file = file.path(
    opt$output,
    paste0(opt$output_prefix,"_actual_results_per_sample_training_data.csv")
  )
)

write_csv(
  x = results_summary_actual_train_data,
  file = file.path(
    opt$output,
    paste0(opt$output_prefix,"_actual_results_summary_training_data.csv")
  )
)

write_csv(
  x = sil_predict_train_df,
  file = file.path(
    opt$output,
    paste0(opt$output_prefix,"_predicted_results_per_sample_training_data.csv")
  )
)

write_csv(
  x = results_summary_predicted_train_data,
  file = file.path(
    opt$output,
    paste0(opt$output_prefix,"_predicted_results_summary_training_data.csv")
  )
)

#plots for predicted test data
res_tbl %>%
  drop_na() %>%
  ggplot(aes(x = as.factor(resolution), y = avg_width)) +
  geom_boxplot() +
  theme_bw() +
  labs(x = "Resolution", y = "Avg. Silhouette Score Across All Cells")

ggsave(filename = paste0(
  opt$output,
  "/",
  opt$output_prefix,
  "_avg_width_boxplot.png"
))

res_tbl %>%
  drop_na() %>%
  ggplot(aes(x = as.factor(resolution), y = cluster_avg_widths)) +
  geom_boxplot() +
  theme_bw() +
  labs(x = "Resolution", y = "Avg. Silhouette Score Across Clusters")

ggsave(filename = paste0(
  opt$output,
  "/",
  opt$output_prefix,
  "_cluster_avg_width_boxplot.png"
))

ggplot(
  results_summary,
  aes(x = as.factor(resolution), y = mean_score, group = 1)
) +
  geom_errorbar(
    aes(
      ymin = mean_score - (1.96*standard_error_score),
      ymax = mean_score + (1.96*standard_error_score),
      width = .3
    ),
    color = "red"
  ) +
  geom_point(colour = "#619CFF") +
  geom_line(colour = "#619CFF") +
  theme_bw() +
  labs(x = "Resolution", y = "Avg. Silhouette Score Across All Cells")

ggsave(filename = paste0(
  opt$output,
  "/",
  opt$output_prefix,
  "_avg_width_95_percent_confidence_interval.png"
))

ggplot(
  results_summary,
  aes(x = as.factor(resolution), y = cluster_mean_score, group = 1) # scale should be the res range
) +
  geom_errorbar(
    aes(
      ymin = cluster_mean_score - (1.96*standard_error_score),
      ymax = cluster_mean_score + (1.96*standard_error_score),
      width = .3
    ),
    color = "red"
  ) +
  geom_point(colour = "#619CFF") +
  geom_line(colour = "#619CFF") +
  theme_bw() +
  labs(x = "Resolution", y = "Avg. Silhouette Score Across Clusters")

ggsave(filename = paste0(
  opt$output,
  "/",
  opt$output_prefix,
  "_cluster_avg_width_95_percent_confidence_interval.png"
))


#plots for predicted training data
sil_predict_train_df %>%
  drop_na() %>%
  ggplot(aes(x = as.factor(resolution), y = mean_avg_width)) +
  geom_boxplot() +
  theme_bw() +
  labs(x = "Resolution", y = "Avg. Silhouette Score Across All Cells")

ggsave(filename = paste0(
  opt$output,
  "/",
  opt$output_prefix,
  "_avg_width_predicted_train_data_per_sample_boxplot.png"
))

sil_predict_train_df %>%
  drop_na() %>%
  ggplot(aes(x = as.factor(resolution), y = mean_cluster_avg_widths)) +
  geom_boxplot() +
  theme_bw() +
  labs(x = "Resolution", y = "Avg. Silhouette Score Across Clusters")

ggsave(filename = paste0(
  opt$output,
  "/",
  opt$output_prefix,
  "_cluster_avg_width_predicted_train_data_per_sample_boxplot.png"
))

ggplot(
  results_summary_predicted_train_data,
  aes(x = as.factor(resolution), y = mean_score, group = 1)
) +
  geom_errorbar(
    aes(
      ymin = mean_score - (1.96*standard_error_score),
      ymax = mean_score + (1.96*standard_error_score),
      width = .3
    ),
    color = "red"
  ) +
  geom_point(colour = "#619CFF") +
  geom_line(colour = "#619CFF") +
  theme_bw() +
  labs(x = "Resolution", y = "Avg. Silhouette Score Across All Cells")

ggsave(filename = paste0(
  opt$output,
  "/",
  opt$output_prefix,
  "_avg_width_predicted_train_data_95_percent_confidence_interval.png"
))

ggplot(
  results_summary_predicted_train_data,
  aes(x = as.factor(resolution), y = cluster_mean_score, group = 1) # scale should be the res range
) +
  geom_errorbar(
    aes(
      ymin = cluster_mean_score - (1.96*standard_error_score),
      ymax = cluster_mean_score + (1.96*standard_error_score),
      width = .3
    ),
    color = "red"
  ) +
  geom_point(colour = "#619CFF") +
  geom_line(colour = "#619CFF") +
  theme_bw() +
  labs(x = "Resolution", y = "Avg. Silhouette Score Across Clusters")

ggsave(filename = paste0(
  opt$output,
  "/",
  opt$output_prefix,
  "_cluster_avg_width_predicted_train_data_95_percent_confidence_interval.png"
))



#plots for actual training data
sil_actual_train_df %>%
  drop_na() %>%
  ggplot(aes(x = as.factor(resolution), y = mean_avg_width)) +
  geom_boxplot() +
  theme_bw() +
  labs(x = "Resolution", y = "Avg. Silhouette Score Across All Cells")

ggsave(filename = paste0(
  opt$output,
  "/",
  opt$output_prefix,
  "_avg_width_actual_train_data_per_sample_boxplot.png"
))

sil_actual_train_df %>%
  drop_na() %>%
  ggplot(aes(x = as.factor(resolution), y = mean_cluster_avg_widths)) +
  geom_boxplot() +
  theme_bw() +
  labs(x = "Resolution", y = "Avg. Silhouette Score Across Clusters")

ggsave(filename = paste0(
  opt$output,
  "/",
  opt$output_prefix,
  "_cluster_avg_width_actual_train_data_per_sample_boxplot.png"
))

ggplot(
  results_summary_actual_train_data,
  aes(x = as.factor(resolution), y = mean_score, group = 1)
) +
  geom_errorbar(
    aes(
      ymin = mean_score - (1.96*standard_error_score),
      ymax = mean_score + (1.96*standard_error_score),
      width = .3
    ),
    color = "red"
  ) +
  geom_point(colour = "#619CFF") +
  geom_line(colour = "#619CFF") +
  theme_bw() +
  labs(x = "Resolution", y = "Avg. Silhouette Score Across All Cells")

ggsave(filename = paste0(
  opt$output,
  "/",
  opt$output_prefix,
  "_avg_width_actual_train_data_95_percent_confidence_interval.png"
))

ggplot(
  results_summary_actual_train_data,
  aes(x = as.factor(resolution), y = cluster_mean_score, group = 1) # scale should be the res range
) +
  geom_errorbar(
    aes(
      ymin = cluster_mean_score - (1.96*standard_error_score),
      ymax = cluster_mean_score + (1.96*standard_error_score),
      width = .3
    ),
    color = "red"
  ) +
  geom_point(colour = "#619CFF") +
  geom_line(colour = "#619CFF") +
  theme_bw() +
  labs(x = "Resolution", y = "Avg. Silhouette Score Across Clusters")

ggsave(filename = paste0(
  opt$output,
  "/",
  opt$output_prefix,
  "_cluster_avg_width_actual_train_data_95_percent_confidence_interval.png"
))

writeLines(
  capture.output(sessionInfo()),
  file.path(opt$output, "sessionInfo.txt")
)